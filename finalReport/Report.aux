\relax 
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{Introduction}{{I}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Background}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Problem Description}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Robocode Environment}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Report Outline}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Potential Fields}{2}}
\newlabel{Potential Fields}{{II}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Setup}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Experiments}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Notice that the blue robot has lost track of the goal and is tracking an older position.}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The blue tracking robot has located the goal with its radar, even though it is facing a different direction. It will never lose track of the goal now that it has a lock.}}{4}}
\newlabel{Radar Lock Example}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The final test for the Potential Field implementation features obstacles, both stationary and moving, and a moving goal.}}{6}}
\newlabel{Radar Lock Example}{{3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Conclusion}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Markov Decision Processes}{8}}
\newlabel{Markov Decision Processes}{{III}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Introduction}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Setup}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Implementation and Experimentation}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Part I - Framework}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces MDP Policy to lead an MDP bot to state 273. The motion of the MDP bot following this policy is assumed to have no noise.}}{10}}
\newlabel{fig:sample}{{4}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces MDP Policy to lead an MDP bot to state 273. The motion of the MDP bot following this policy is assumed to follow the noise model described above.}}{11}}
\newlabel{fig:sample}{{5}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Part II - Runloop Integrated Value Iteration}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Part III - Moving Target Tracking}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}Part IV - Moving Target Tracking with Real-Time Optimizations}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces MDP Policy that has been updated from goal state 190 to goal state 150 using our real-time value iteration function.}}{14}}
\newlabel{fig:sample}{{6}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces MDP Policy that has been updated from goal state 25 to goal state 31. With this large of a jump between goal states, the real-time value iteration algorithm fails.}}{16}}
\newlabel{fig:sample}{{7}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5}Part V - Dynamic Obstacle Avoidance}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces MDP Policy to lead an MDP bot to state 0 while avoiding obstacles at states 67 and 310.}}{18}}
\newlabel{fig:sample}{{8}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces MDP Policy to lead an MDP bot to state 68 while avoiding obstacles at states 67 and 310. The goal state is too close to the obstacles, and the path to the goal is obliterated.}}{19}}
\newlabel{fig:sample}{{9}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Analysis}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Q-learning}{21}}
\newlabel{Q-learning}{{IV}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Introduction}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Setup}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Experiments}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Offline trials}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Taking it online}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Performance and dynamic environments}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Basic Q-learning generated policy.}}{23}}
\newlabel{fig:basicQ}{{10}{23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}Obstacles}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Q-learning generated policy with wall present at $x=10$.}}{24}}
\newlabel{fig:obstacleQ}{{11}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5}Heuristic reward functions}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Q-learning generated policy with a heuristic reward policy.}}{26}}
\newlabel{fig:heuristicQ}{{12}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Evaluation}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{28}}
\newlabel{Conclusion}{{V}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Future work}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Environment and model complexity}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Extensions of existing methods}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Algorithmic research}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Acknowledgments}{30}}
\newlabel{LastPage}{{}{30}}
