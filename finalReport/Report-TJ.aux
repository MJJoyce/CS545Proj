\relax 
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{Introduction}{{I}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Background}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Problem Description}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Robocode Environment}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Report Outline}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Potential Fields}{1}}
\newlabel{Potential Fields}{{II}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Markov Decision Processes}{2}}
\newlabel{Markov Decision Processes}{{III}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Introduction}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Setup}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Implementation and Experimentation}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Part I - Framework}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces MDP Policy to lead an MDP bot to state 273. The motion of the MDP bot following this policy is assumed to have no noise.}}{4}}
\newlabel{fig:sample}{{1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces MDP Policy to lead an MDP bot to state 273. The motion of the MDP bot following this policy is assumed to follow the noise model described above.}}{5}}
\newlabel{fig:sample}{{2}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Part II - Runloop Integrated Value Iteration}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Part III - Moving Target Tracking}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}Part IV - Moving Target Tracking with Real-Time Optimizations}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces MDP Policy that has been updated from goal state 190 to goal state 150 using our real-time value iteration function.}}{8}}
\newlabel{fig:sample}{{3}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces MDP Policy that has been updated from goal state 25 to goal state 31. With this large of a jump between goal states, the real-time value iteration algorithm fails.}}{10}}
\newlabel{fig:sample}{{4}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5}Part V - Dynamic Obstacle Avoidance}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces MDP Policy to lead an MDP bot to state 0 while avoiding obstacles at states 67 and 310.}}{12}}
\newlabel{fig:sample}{{5}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces MDP Policy to lead an MDP bot to state 68 while avoiding obstacles at states 67 and 310. The goal state is too close to the obstacles, and the path to the goal is obliterated.}}{13}}
\newlabel{fig:sample}{{6}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Analysis}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Q-learning}{15}}
\newlabel{Q-learning}{{IV}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{15}}
\newlabel{Conclusion}{{V}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Future work}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Acknowledgments}{15}}
\newlabel{LastPage}{{}{15}}
\xdef\lastpage@lastpage{15}
\gdef\lastpage@lastpageHy{}
